{
 "metadata": {
  "name": "",
  "signature": "sha256:85717d73719ce1e93ad907d17fc0954d0f432f389b6795aebca246eaecc2a4fb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Prework:\n",
      "=======\n",
      "Go to http://spark.apache.org/downloads.html \n",
      "\n",
      "Download Spark: spark-1.2.0.tgz\n",
      "\n",
      "Move it somewhere where you won\u2019t lose it and decompress.\n",
      "\n",
      "\n",
      "Follow the instructions at https://spark.apache.org/docs/latest/ec2-scripts.html:\n",
      "```bash\n",
      "export AWS_ACCESS_KEY_ID=AWS_ACCESS_KEY_ID\n",
      "export AWS_SECRET_ACCESS_KEY=AWS_SECRET_ACCESS_KEY\n",
      "\n",
      "./spark-ec2 launch -k key -i KEY_LOCATION -s 5 \u2014t m3.xlarge my_cluster --copy-aws-credentials\n",
      "./spark-ec2 -i KEY_LOCATION login test_cluster\n",
      "```\n",
      "\n",
      "*N.B. be sure to include* **`--copy-aws-credentials`** *when launching your cluster this time!*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once it's up:\n",
      "```bash\n",
      "./spark-ec2 -i KEY_LOCATION login test_cluster\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Word Count in Spark:\n",
      "-------------------\n",
      "Following example adapted from http://spark.apache.org/docs/1.2.0/quick-start.html:\n",
      "```bash\n",
      "spark/bin/pyspark\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once loaded, import numpy with the following:\n",
      "```python\n",
      "import numpy as np\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "then try:\n",
      "```python\n",
      "gutenberg = sc.textFile(\"s3n://DAT6/gutenberg/\")\n",
      "gutenberg.first()\n",
      "```\n",
      "after a couple dozen log lines, you should see the output:\n",
      "\n",
      "`u'\\ufeffThe Project Gutenberg EBook of The Outline of Science, Vol. 1 (of 4), by '`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try the word count example on these data:\n",
      "```python\n",
      "words = gutenberg.flatMap(lambda line: line.split())\n",
      "word_tuples = words.map(lambda word: (word, 1))\n",
      "word_counts = word_tuples.reduceByKey(lambda a, b: a+b)\n",
      "```\n",
      "PySpark doesn't have an easy way to plot, but we can at least print out the values for a histogram:\n",
      "```python\n",
      "word_counts.values().histogram(map(int, np.logspace(0,6,7)))\n",
      "```\n",
      "We can also look at the top ten most common words by using the second value in the tuple as the sort key:\n",
      "```python\n",
      "word_counts.top(10, key = lambda x: x[1])\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-------------------\n",
      "Spark SQL in Action:\n",
      "-------------------\n",
      "Following example adapted from http://spark.apache.org/docs/1.2.0/sql-programming-guide.html:\n",
      "```python\n",
      "from datetime import datetime  # we will want the datetime object later\n",
      "from pyspark.sql import SQLContext, Row\n",
      "sqlContext = SQLContext(sc)\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load the offers csv into an RDD using the following code to split and parse it:\n",
      "```python\n",
      "# Load a text file and convert each line to a dictionary.\n",
      "offers_lines = sc.textFile(\"s3n://dat-sf-12/offers/offers.csv\").filter(lambda x: x[:2] != 'of')\n",
      "offers_parts = offers_lines.map(lambda l: l.split(\",\"))\n",
      "```\n",
      "Test it:\n",
      "```python\n",
      "offers_parts.count()\n",
      "offers_parts.first()\n",
      "```\n",
      "#### Question: why do you suppose we added a filter?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use the following to create a SQL table from the RDD:\n",
      "```python\n",
      "offers = offers_parts.map(lambda p: Row(offer = int(p[0]), category = int(p[1]), quantity = int(p[2]), \n",
      "\tcompany = int(p[3]), offervalue = float(p[4]), brand = int(p[5])))\n",
      "\n",
      "# Infer the schema, and register the SchemaRDD as a table.\n",
      "schemaOffers = sqlContext.inferSchema(offers)\n",
      "schemaOffers.registerTempTable(\"offers\")\n",
      "```\n",
      "Test it:\n",
      "```python\n",
      "# SQL can be run over SchemaRDDs that have been registered as a table.\n",
      "sqlContext.sql(\"SELECT * FROM offers LIMIT 10\").collect()\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In Pairs:\n",
      "--------\n",
      "\n",
      "#### 1. Do the same with `s3n://dat-sf-12/history/trainHistory.csv`\n",
      "\n",
      "Hint, here's the schema:\n",
      "```python\n",
      "Row(id = int(p[0]), chain = int(p[1]), offer = int(p[2]), market = int(p[3]), \n",
      "\trepeattrips = int(p[4]), repeater = bool(p[5]), offerdate = datetime.strptime(p[6], '%Y-%m-%d'))\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Test it:\n",
      "```python\n",
      "recent_offers = sqlContext.sql(\"\"\"SELECT offerdate, id, offervalue \n",
      "    FROM offers JOIN history ON offers.offer = history.offer \n",
      "    ORDER BY offerdate DESC\"\"\")\n",
      "\n",
      "for row in recent_offers.top(10):\n",
      "    '${offervalue:0.2f} off for customer {id} on {offerdate:%m/%d/%Y}'.format(**row.asDict())\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "you should see something like:\n",
      "```\n",
      "'$2.00 off for customer 2118659196 on 04/30/2013'\n",
      "'$2.00 off for customer 2115931556 on 04/30/2013'\n",
      "'$2.00 off for customer 2089678223 on 04/30/2013'\n",
      "'$2.00 off for customer 2073228907 on 04/30/2013'\n",
      "'$2.00 off for customer 2071978226 on 04/30/2013'\n",
      "'$2.00 off for customer 2070014653 on 04/30/2013'\n",
      "'$3.00 off for customer 2056836240 on 04/30/2013'\n",
      "'$2.00 off for customer 2009529994 on 04/30/2013'\n",
      "'$2.00 off for customer 2006719518 on 04/30/2013'\n",
      "'$3.00 off for customer 2006134668 on 04/30/2013'\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 2. (optional) Try loading (just) `s3n://dat-sf-12/transactions/trans-aa` and joining it with the above tables."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 3. Back to the `gutenberg` data, can you change the function in `.flatMap()` to output *bigrams* (that is, word pairs) instead of single tokens?\n",
      "\n",
      "*e.g.* '`Project Gutenberg EBook of The Outline of Science`' would produce:\n",
      "\n",
      "    ['Project Gutenberg', 'Gutenberg EBook', 'EBook of', 'of The', 'The Outline', 'Outline of', 'of Science']\n",
      "\n",
      "**Then count and sort those bigrams.**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 4. (optional) On your own, if you wish, try loading all of `s3n://dat-sf-12/transactions/`. \n",
      "\n",
      "Warning: This takes ~2 hours on 5 m3.xlarge nodes. You may wish to use the [`.sample()`](http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sample) function to speed it up once it's loaded."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Further reading:\n",
      "If you want to learn how to use PySpark in IPython Notebook, look at http://nbviewer.ipython.org/gist/fperez/6384491/00-Setup-IPython-PySpark.ipynb and http://blog.cloudera.com/blog/2014/08/how-to-use-ipython-notebook-with-apache-spark/. It's a little tricky but may be worth the effort if you anticipate doing a lot of data exploration with Spark."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "from __future__ import division\n",
      "\n",
      "import pandas as pd\n",
      "import scipy.stats as stats\n",
      "import seaborn as sns\n",
      "import statsmodels.formula.api as smf\n",
      "from seaborn import plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = pd.read_excel('C:\\GADS\\Game_by_Game_Data.xlsx')\n",
      "data.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Year</th>\n",
        "      <th>Team</th>\n",
        "      <th>Current Conference</th>\n",
        "      <th>Actual Conference</th>\n",
        "      <th>Team State</th>\n",
        "      <th>Team City</th>\n",
        "      <th>Coach</th>\n",
        "      <th>Ranking</th>\n",
        "      <th>Seed</th>\n",
        "      <th>RPI</th>\n",
        "      <th>...</th>\n",
        "      <th>Opponent Seed</th>\n",
        "      <th>Opponent RPI</th>\n",
        "      <th>Opponent Region</th>\n",
        "      <th>Location City</th>\n",
        "      <th>Location State</th>\n",
        "      <th>Day of Week</th>\n",
        "      <th>Date</th>\n",
        "      <th>Decade</th>\n",
        "      <th>Wins</th>\n",
        "      <th>Losses</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 2014</td>\n",
        "      <td>         Texas Southern</td>\n",
        "      <td> Southwestern Athletic</td>\n",
        "      <td> Southwestern Athletic</td>\n",
        "      <td>          Texas</td>\n",
        "      <td>     Houston</td>\n",
        "      <td>    Davis, Mike</td>\n",
        "      <td>  0</td>\n",
        "      <td> 16</td>\n",
        "      <td> 239</td>\n",
        "      <td>...</td>\n",
        "      <td> 16</td>\n",
        "      <td> 208</td>\n",
        "      <td> Midwest</td>\n",
        "      <td>       Dayton</td>\n",
        "      <td>      Ohio</td>\n",
        "      <td> Wednesday</td>\n",
        "      <td> March 19</td>\n",
        "      <td> 2010-2019</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 2014</td>\n",
        "      <td>                   UCLA</td>\n",
        "      <td>        Pacific Twelve</td>\n",
        "      <td>        Pacific Twelve</td>\n",
        "      <td>     California</td>\n",
        "      <td> Los Angeles</td>\n",
        "      <td>  Alford, Steve</td>\n",
        "      <td> 20</td>\n",
        "      <td>  4</td>\n",
        "      <td>  14</td>\n",
        "      <td>...</td>\n",
        "      <td>  1</td>\n",
        "      <td>   1</td>\n",
        "      <td>   South</td>\n",
        "      <td>      Memphis</td>\n",
        "      <td> Tennessee</td>\n",
        "      <td>  Thursday</td>\n",
        "      <td> March 27</td>\n",
        "      <td> 2010-2019</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 2014</td>\n",
        "      <td> North Carolina Central</td>\n",
        "      <td>           Mid Eastern</td>\n",
        "      <td>            Mideastern</td>\n",
        "      <td> North Carolina</td>\n",
        "      <td>      Durham</td>\n",
        "      <td> Moton, LeVelle</td>\n",
        "      <td>  0</td>\n",
        "      <td> 14</td>\n",
        "      <td>  99</td>\n",
        "      <td>...</td>\n",
        "      <td>  3</td>\n",
        "      <td>   7</td>\n",
        "      <td>    East</td>\n",
        "      <td>  San Antonio</td>\n",
        "      <td>     Texas</td>\n",
        "      <td>    Friday</td>\n",
        "      <td> March 21</td>\n",
        "      <td> 2010-2019</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 2014</td>\n",
        "      <td>             Louisville</td>\n",
        "      <td>        Atlantic Coast</td>\n",
        "      <td>     American Athletic</td>\n",
        "      <td>       Kentucky</td>\n",
        "      <td>  Louisville</td>\n",
        "      <td>   Pitino, Rick</td>\n",
        "      <td>  5</td>\n",
        "      <td>  4</td>\n",
        "      <td>  19</td>\n",
        "      <td>...</td>\n",
        "      <td>  8</td>\n",
        "      <td>  17</td>\n",
        "      <td> Midwest</td>\n",
        "      <td> Indianapolis</td>\n",
        "      <td>   Indiana</td>\n",
        "      <td>    Friday</td>\n",
        "      <td> March 28</td>\n",
        "      <td> 2010-2019</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 2014</td>\n",
        "      <td>             New Mexico</td>\n",
        "      <td>         Mountain West</td>\n",
        "      <td>         Mountain West</td>\n",
        "      <td>     New Mexico</td>\n",
        "      <td> Albuquerque</td>\n",
        "      <td>    Neal, Craig</td>\n",
        "      <td> 17</td>\n",
        "      <td>  7</td>\n",
        "      <td>  12</td>\n",
        "      <td>...</td>\n",
        "      <td> 10</td>\n",
        "      <td>  41</td>\n",
        "      <td>   South</td>\n",
        "      <td>    St. Louis</td>\n",
        "      <td>  Missouri</td>\n",
        "      <td>    Friday</td>\n",
        "      <td> March 21</td>\n",
        "      <td> 2010-2019</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 36 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "   Year                    Team     Current Conference      Actual Conference  \\\n",
        "0  2014          Texas Southern  Southwestern Athletic  Southwestern Athletic   \n",
        "1  2014                    UCLA         Pacific Twelve         Pacific Twelve   \n",
        "2  2014  North Carolina Central            Mid Eastern             Mideastern   \n",
        "3  2014              Louisville         Atlantic Coast      American Athletic   \n",
        "4  2014              New Mexico          Mountain West          Mountain West   \n",
        "\n",
        "       Team State    Team City           Coach  Ranking  Seed  RPI   ...    \\\n",
        "0           Texas      Houston     Davis, Mike        0    16  239   ...     \n",
        "1      California  Los Angeles   Alford, Steve       20     4   14   ...     \n",
        "2  North Carolina       Durham  Moton, LeVelle        0    14   99   ...     \n",
        "3        Kentucky   Louisville    Pitino, Rick        5     4   19   ...     \n",
        "4      New Mexico  Albuquerque     Neal, Craig       17     7   12   ...     \n",
        "\n",
        "   Opponent Seed  Opponent RPI Opponent Region Location City Location State  \\\n",
        "0             16           208         Midwest        Dayton           Ohio   \n",
        "1              1             1           South       Memphis      Tennessee   \n",
        "2              3             7            East   San Antonio          Texas   \n",
        "3              8            17         Midwest  Indianapolis        Indiana   \n",
        "4             10            41           South     St. Louis       Missouri   \n",
        "\n",
        "   Day of Week      Date     Decade  Wins  Losses  \n",
        "0    Wednesday  March 19  2010-2019     0       1  \n",
        "1     Thursday  March 27  2010-2019     0       1  \n",
        "2       Friday  March 21  2010-2019     0       1  \n",
        "3       Friday  March 28  2010-2019     0       1  \n",
        "4       Friday  March 21  2010-2019     0       1  \n",
        "\n",
        "[5 rows x 36 columns]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'ExcelFile' object has no attribute 'head'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-4-199065406915>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mAttributeError\u001b[0m: 'ExcelFile' object has no attribute 'head'"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}